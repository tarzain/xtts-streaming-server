# How to run the server
1. Download your model to the `models/` directory
2. run `python3 -m llama_cpp.server --model path/to/model.gguf`